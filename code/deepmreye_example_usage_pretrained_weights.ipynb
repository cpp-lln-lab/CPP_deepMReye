{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.6.3.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:70% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.output_result { max-width:70% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from bids import BIDSLayout\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from deepmreye import architecture, train, analyse, preprocess\n",
    "from deepmreye.util import util, data_generator, model_opts\n",
    "\n",
    "from plotly.offline import init_notebook_mode\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Initialize plotly figures\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Make sure the output width is adjusted for better export as HTML\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_result { max-width:70% !important; }</style>\"))\n",
    "\n",
    "# Change to os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" if you dont have access to a GP\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# Autoreload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# lint with black\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = \"../inputs/models/\"\n",
    "experiment_folder = \"../inputs/rest_blnd_can_fmriprep/\"\n",
    "\n",
    "task = \"rest\"\n",
    "suffix = \"bold\"\n",
    "extension = \"nii.gz\"\n",
    "space = \"MNI152NLin2009cAsym\"\n",
    "\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = BIDSLayout(experiment_folder, validate=False, derivatives=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File ../deepmreye/masks/eyemask_small.nii does not exist!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12558/104369998.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Preload masks to save time within participant loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0meyemask_small\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meyemask_big\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdme_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_edges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_edges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_edges\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meyemask_small\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/CPP_deepMReye/code/env/lib/python3.8/site-packages/deepmreye/preprocess.py\u001b[0m in \u001b[0;36mget_masks\u001b[0;34m(data_path)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mEdges\u001b[0m \u001b[0mof\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \"\"\"     \n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0meyemask_small\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'eyemask_small.nii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0meyemask_big\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'eyemask_big.nii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mdme_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'dme_template.nii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/CPP_deepMReye/code/env/lib/python3.8/site-packages/ants/core/ants_image_io.py\u001b[0m in \u001b[0;36mimage_read\u001b[0;34m(filename, dimension, pixeltype, reorient)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File %s does not exist!\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0mhinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_header_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: File ../deepmreye/masks/eyemask_small.nii does not exist!"
     ]
    }
   ],
   "source": [
    "# Preload masks to save time within participant loop\n",
    "(eyemask_small, eyemask_big, dme_template, mask, x_edges, y_edges, z_edges) = preprocess.get_masks()\n",
    "\n",
    "print(eyemask_small)\n",
    "\n",
    "subjects = layout.get(return_type='id', target='subject')\n",
    "\n",
    "if debug:\n",
    "    subjects = [subjects[0]]\n",
    "    \n",
    "for sub in subjects:\n",
    "    print(f\"Running participant: {sub}\")\n",
    "    bf = layout.get(return_type='filename', subject=sub, suffix=suffix, task=task, extension=extension, space=space)\n",
    "    print(f\"Input file: {bf}\")\n",
    "    preprocess.run_participant(fp_func, dme_template, eyemask_big, eyemask_small, x_edges, y_edges, z_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preload masks to save time within participant loop\n",
    "(eyemask_small, eyemask_big, dme_template, mask, x_edges, y_edges, z_edges) = preprocess.get_masks()\n",
    "\n",
    "# Loop across participants and extract eye mask\n",
    "for participant in participants:\n",
    "    if participant.startswith('s'):\n",
    "        print('Running participant {}'.format(participant))\n",
    "        participant_folder = functional_data + participant\n",
    "        for run in os.listdir(participant_folder):\n",
    "            if run.startswith('run'):\n",
    "                fp_func = participant_folder + os.path.sep + run # Filepath to functional\n",
    "                preprocess.run_participant(fp_func, dme_template, eyemask_big, eyemask_small, x_edges, y_edges, z_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the pipeline produces diagnostic figures that can and should be carefully examined for each individual scanning run. These figures are saved as interactive html file together with the rest of the model output. Here is one such [html file](https://github.com/DeepMReye/DeepMReye/blob/main/tests/data/report_test_participant.html) as an example. If the alignment is off, try running above loop with:\n",
    "```python\n",
    "preprocess.run_participant(fp_func, dme_template, eyemask_big, eyemask_small, x_edges, y_edges, z_edges, transforms=['Affine', 'Affine', 'SyNAggro'])\n",
    "```\n",
    "which uses a more aggressive (and non-linear) alignment procedure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine processed masks with labels\n",
    "for participant in participants:\n",
    "    if participant.startswith('s'):\n",
    "        print('Running participant {}'.format(participant))\n",
    "        participant_folder = functional_data + participant\n",
    "        participant_data, participant_labels, participant_ids = [], [], []\n",
    "        for run_idx, run in enumerate(os.listdir(participant_folder)):\n",
    "            if not run.endswith(\".p\"):\n",
    "                continue\n",
    "            # Load mask and normalize it\n",
    "            this_mask = participant_folder + os.path.sep + run\n",
    "            this_mask = pickle.load(open(this_mask, 'rb'))\n",
    "            this_mask = preprocess.normalize_img(this_mask)\n",
    "        \n",
    "            # If experiment has no labels use dummy labels\n",
    "            this_label = np.zeros((this_mask.shape[3], 10, 2)) # 10 is the number of subTRs used in the pretrained weights, 2 is XY\n",
    "        \n",
    "            # Check if each functional image has a corresponding label. Note that mask has time as third dimension\n",
    "            if not this_mask.shape[3] == this_label.shape[0]:\n",
    "                print('WARNING --- Skipping Subject {} Run {} --- Wrong alignment (Mask {} - Label {}).'.format(subj, run_number, this_mask.shape, this_label.shape))\n",
    "                continue\n",
    "        \n",
    "            # Store across runs\n",
    "            participant_data.append(this_mask)\n",
    "            participant_labels.append(this_label)\n",
    "            participant_ids.append(([participant]*this_label.shape[0], [run_idx]*this_label.shape[0]))\n",
    "    \n",
    "        # Save participant file\n",
    "        preprocess.save_data(participant + 'no_label', participant_data, participant_labels, participant_ids, processed_data, center_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "<a name=\"load\"></a>\n",
    "## Load & visualize input\n",
    "\n",
    "Here, we showcase the input / output pairs that DeepMReye uses to decode position from the eye balls of the participant by visualizing the input of the model in the left panel ('Normalized MR-Signal') and the training labels in the right panel ('Gaze position')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define paths to example dataset\n",
    "datasets = [processed_data + p for p in os.listdir(processed_data) if 'no_label' in p]\n",
    "\n",
    "# Load data from one participant to showcase input/output\n",
    "X, y = data_generator.get_all_subject_data(datasets[0])\n",
    "print('Input: {}, Output: {}'.format(X.shape, y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = analyse.visualise_input_data(X, y, bg_color=\"rgb(255,255,255)\", ylim=[-11, 11])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows a slice through the coregistered eyeballs of an exemplary participant (left panel) as well as the corresponding horizontal (X) and vertical (Y) gaze position over time. The normalized MRI signal was color coded (red = strong, black = weak MRI signal relative to baseline). The slider at the bottom allows exploring the eyeball-MRI signal and its relationship to gaze position over time (i.e. functional images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "<a name=\"train\"></a>\n",
    "## Load model weights\n",
    "Here we use a pretrained model to decode gaze position from the example dataset. Model weights for all datasets and combined model runs can be downloaded under model_weights here: [https://osf.io/mrhk9/](https://osf.io/mrhk9/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "opts = model_opts.get_opts()\n",
    "test_participants = [processed_data + p for p in os.listdir(processed_data) if 'no_label' in p]\n",
    "generators = data_generator.create_generators(test_participants, test_participants)\n",
    "generators = (*generators, test_participants, test_participants) # Add participant list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model_weights = model_weights + 'datasets_1to5.h5'\n",
    "\n",
    "# Get untrained model and load with trained weights\n",
    "(model, model_inference) = train.train_model(dataset='example_data', generators=generators, opts=opts, return_untrained=True)\n",
    "model_inference.load_weights(model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "<a name=\"evaluate\"></a>\n",
    "## Model evaluation\n",
    "\n",
    "We can now evaluate all participants on the trained weights. \n",
    "Verbosity can be set here to get a more fine-grained detail of the model performance over different evaluation metrics (Pearson, [R^2-Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html), Euclidean Error). We then plot the group results for different metrics as a boxplot (left, hovering across single dots indicates participant identifier) and show the real and predicted gaze position as a line plot to the right split into X and Y component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "(evaluation, scores) = train.evaluate_model(dataset='example_data', model=model_inference, generators=generators,\n",
    "                                            save=False, model_path=experiment_folder, model_description='', verbose=2, percentile_cut=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = analyse.visualise_predictions_slider(evaluation, scores, color=\"rgb(0, 150, 175)\", bg_color=\"rgb(255,255,255)\", ylim=[-11, 11])\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a75cf8d77803a3f6007b5235f3728b02b0be60187c0e445555102b8493d0216"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
